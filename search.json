[
  {
    "objectID": "Assignment3.html",
    "href": "Assignment3.html",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "",
    "text": "# Sample program for using quanteda for text modeling and analysis\n# Documentation: vignette(\"quickstart\", package = \"quanteda\")\n# Website: https://quanteda.io/\n\n\nlibrary(quanteda)\n\nPackage version: 4.1.0\nUnicode version: 14.0\nICU version: 71.1\n\n\nParallel computing: disabled\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Twitter data about President Biden and Xi summit in Novemeber 2021\n# Do some background search/study on the event\n# \nsummit <- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\nRows: 14520 Columns: 90\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nView(summit)\n\nsum_twt = summit$text\ntoks = tokens(sum_twt)\nsumtwtdfm <- dfm(toks)\nclass(toks)\n\n[1] \"tokens\"\n\n# Latent Semantic Analysis \n## (https://quanteda.io/reference/textmodel_lsa.html)\n\nsum_lsa <- textmodel_lsa(sumtwtdfm, nd=4,  margin = c(\"both\", \"documents\", \"features\"))\nsummary(sum_lsa)\n\n                Length    Class     Mode   \nsk                      4 -none-    numeric\ndocs                58080 -none-    numeric\nfeatures            63972 -none-    numeric\nmatrix_low_rank 232218360 -none-    numeric\ndata            232218360 dgCMatrix S4     \n\nhead(sum_lsa$docs)\n\n              [,1]          [,2]          [,3]          [,4]\ntext1 8.670375e-03  9.539431e-03 -3.365261e-03  1.378640e-02\ntext2 8.662406e-06 -8.754517e-06 -6.159723e-06  1.673892e-05\ntext3 2.917454e-03  6.809891e-03  1.059921e-03 -3.180288e-03\ntext4 1.046103e-02  8.782783e-04 -4.359418e-03  4.941183e-03\ntext5 3.247147e-03  8.006068e-03  1.632191e-04 -4.657788e-03\ntext6 3.247147e-03  8.006068e-03  1.632191e-04 -4.657788e-03\n\nclass(sum_lsa)\n\n[1] \"textmodel_lsa\"\n\ntweet_dfm <- tokens(sum_twt, remove_punct = TRUE) %>%\n  dfm()\nhead(tweet_dfm)\n\nDocument-feature matrix of: 6 documents, 15,927 features (99.89% sparse) and 0 docvars.\n       features\ndocs    breaking news us president biden amp communist china leader xi\n  text1        1    1  1         1     1   1         1     2      1  1\n  text2        0    0  0         0     0   0         0     0      0  0\n  text3        0    0  0         0     1   0         0     0      0  1\n  text4        0    0  0         1     1   0         0     0      0  1\n  text5        0    0  0         0     1   0         0     0      0  1\n  text6        0    0  0         0     1   0         0     0      0  1\n[ reached max_nfeat ... 15,917 more features ]\n\ntag_dfm <- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag <- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n [1] \"#china\"       \"#biden\"       \"#xijinping\"   \"#joebiden\"    \"#america\"    \n [6] \"#americans\"   \"#coronavirus\" \"#fentanyl\"    \"#xi\"          \"#us\"         \n\nlibrary(\"quanteda.textplots\")\ntag_fcm <- fcm(tag_dfm)\nhead(tag_fcm)\n\nFeature co-occurrence matrix of: 6 by 665 features.\n               features\nfeatures        #breaking #breakingnews #biden #china #usa #pray4america\n  #breaking             0             4      5      5    5             0\n  #breakingnews         0             0      4      5    4             0\n  #biden                0             0      0    443   49             0\n  #china                0             0      0      8   76             0\n  #usa                  0             0      0      0    6             0\n  #pray4america         0             0      0      0    0             0\n               features\nfeatures        #joebiden #xijinping #america #americans\n  #breaking             0          0        0          0\n  #breakingnews         0          0        0          0\n  #biden              299        370      302        295\n  #china              339        434      308        295\n  #usa                 12         15        0          0\n  #pray4america         0          0        0          0\n[ reached max_nfeat ... 655 more features ]\n\ntopgat_fcm <- fcm_select(tag_fcm, pattern = toptag)\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\nuser_dfm <- dfm_select(tweet_dfm, pattern = \"@*\")\ntopuser <- names(topfeatures(user_dfm, 50))\nhead(topuser, 20)\n\n [1] \"@potus\"           \"@politico\"        \"@joebiden\"        \"@jendeben\"       \n [5] \"@eneskanter\"      \"@nwadhams\"        \"@phelimkine\"      \"@nahaltoosi\"     \n [9] \"@nba\"             \"@washwizards\"     \"@pelicansnba\"     \"@capitalonearena\"\n[13] \"@kevinliptakcnn\"  \"@foxbusiness\"     \"@morningsmaria\"   \"@scmpnews\"       \n[17] \"@petermartin_pcm\" \"@nytimes\"         \"@uyghur_american\" \"@kaylatausche\"   \n\nuser_fcm <- fcm(user_dfm)\nhead(user_fcm, 20)\n\nFeature co-occurrence matrix of: 20 by 711 features.\n                 features\nfeatures          @youtube @bfmtv @cnn @lauhaim @barackobama @joebiden\n  @youtube               0      0    0        0            0         0\n  @bfmtv                 0      0    1        1            1         1\n  @cnn                   0      0    0        1            1         1\n  @lauhaim               0      0    0        0            1         1\n  @barackobama           0      0    0        0            0         1\n  @joebiden              0      0    0        0            0         3\n  @kamalaharris          0      0    0        0            0         0\n  @hillaryclinton        0      0    0        0            0         0\n  @billclinton           0      0    0        0            0         0\n  @cbsnews               0      0    0        0            0         0\n                 features\nfeatures          @kamalaharris @hillaryclinton @billclinton @cbsnews\n  @youtube                    0               0            0        0\n  @bfmtv                      1               1            1        1\n  @cnn                        1               1            1        1\n  @lauhaim                    1               1            1        1\n  @barackobama                1               1            1        1\n  @joebiden                   1               1            1        1\n  @kamalaharris               0               1            1        1\n  @hillaryclinton             0               0            1        1\n  @billclinton                0               0            0        1\n  @cbsnews                    0               0            0        0\n[ reached max_feat ... 10 more features, reached max_nfeat ... 701 more features ]\n\nuser_fcm <- fcm_select(user_fcm, pattern = topuser)\ntextplot_network(user_fcm, min_freq = 20, edge_color = \"firebrick\", edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n# Sample program for using quanteda for text modeling and analysis\n# Documentation: vignette(\"quickstart\", package = c(\"quanteda.textstats\", \"quanteda.textmodels\")\n# Website: https://quanteda.io/\n\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Wordcloud\n# based on US presidential inaugural address texts, and metadata (for the corpus), from 1789 to present.\ndfm_inaug <- corpus_subset(data_corpus_inaugural, Year <= 1826) %>% \n  tokens(remove_punct = TRUE) %>% \n  tokens_remove(stopwords('english')) %>% \n  dfm() %>%\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\nset.seed(100)\ntextplot_wordcloud(dfm_inaug)\n\n\n\ninaug_speech = data_corpus_inaugural\n\ncorpus_subset(data_corpus_inaugural, \n              President %in% c(\"Trump\", \"Obama\", \"Bush\")) %>%\n  tokens(remove_punct = TRUE) %>%\n  tokens_remove(stopwords(\"english\")) %>%\n  dfm() %>%\n  dfm_group(groups = President) %>%\n  dfm_trim(min_termfreq = 5, verbose = FALSE) %>%\n  textplot_wordcloud(comparison = TRUE)\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nthroughout could not be fit on page. It will not be plotted.\n\n\nWarning in wordcloud_comparison(x, min_size, max_size, min_count, max_words, :\nchildren could not be fit on page. It will not be plotted.\n\n\n\n\ntextplot_wordcloud(dfm_inaug, min_count = 10,\n                   color = c('red', 'pink', 'green', 'purple', 'orange', 'blue'))\n\n\n\ndata_corpus_inaugural_subset <- \n  corpus_subset(data_corpus_inaugural, Year > 1949)\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"american\") %>%\n  textplot_xray()\n\n\n\ntextplot_xray(\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"american\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"people\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"communist\")\n  \n)\n\n\n\n## Why is the \"communist\" plot missing?\n\ntheme_set(theme_bw())\ng <- textplot_xray(\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"american\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"people\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"communist\")\n)\ng + aes(color = keyword) + \n  scale_color_manual(values = c(\"blue\", \"red\", \"green\")) +\n  theme(legend.position = \"none\")\n\n\n\nlibrary(quanteda.textstats)\nfeatures_dfm_inaug <- textstat_frequency(dfm_inaug, n = 100)\n\n# Sort by reverse frequency order\nfeatures_dfm_inaug$feature <- with(features_dfm_inaug, reorder(feature, -frequency))\n\nggplot(features_dfm_inaug, aes(x = feature, y = frequency)) +\n  geom_point() + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n# Get frequency grouped by president\nfreq_grouped <- textstat_frequency(dfm(tokens(data_corpus_inaugural_subset)), \n                                   groups = data_corpus_inaugural_subset$President)\n\n# Filter the term \"american\"\nfreq_american <- subset(freq_grouped, freq_grouped$feature %in% \"american\")  \n\nggplot(freq_american, aes(x = group, y = frequency)) +\n  geom_point() + \n  scale_y_continuous(limits = c(0, 14), breaks = c(seq(0, 14, 2))) +\n  xlab(NULL) + \n  ylab(\"Frequency\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\ndfm_rel_freq <- dfm_weight(dfm(tokens(data_corpus_inaugural_subset)), scheme = \"prop\") * 100\nhead(dfm_rel_freq)\n\nDocument-feature matrix of: 6 documents, 4,346 features (85.57% sparse) and 4 docvars.\n                 features\ndocs                      my    friends        ,    before          i\n  1953-Eisenhower 0.14582574 0.14582574 4.593511 0.1822822 0.10936930\n  1957-Eisenhower 0.20975354 0.10487677 6.345045 0.1573152 0.05243838\n  1961-Kennedy    0.19467878 0.06489293 5.451006 0.1297859 0.32446463\n  1965-Johnson    0.17543860 0.05847953 5.555556 0.2339181 0.87719298\n  1969-Nixon      0.28973510 0          5.546358 0.1241722 0.86920530\n  1973-Nixon      0.05012531 0.05012531 4.812030 0.2005013 0.60150376\n                 features\ndocs                   begin      the expression       of     those\n  1953-Eisenhower 0.03645643 6.234050 0.03645643 5.176814 0.1458257\n  1957-Eisenhower 0          5.977976 0          5.034085 0.1573152\n  1961-Kennedy    0.19467878 5.580792 0          4.218040 0.4542505\n  1965-Johnson    0          4.502924 0          3.333333 0.1754386\n  1969-Nixon      0          5.629139 0          3.890728 0.4552980\n  1973-Nixon      0          4.160401 0          3.408521 0.3007519\n[ reached max_nfeat ... 4,336 more features ]\n\nrel_freq <- textstat_frequency(dfm_rel_freq, groups = dfm_rel_freq$President)\n\n# Filter the term \"american\"\nrel_freq_american <- subset(rel_freq, feature %in% \"american\")  \n\nggplot(rel_freq_american, aes(x = group, y = frequency)) +\n  geom_point() + \n  scale_y_continuous(limits = c(0, 0.7), breaks = c(seq(0, 0.7, 0.1))) +\n  xlab(NULL) + \n  ylab(\"Relative frequency\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\ndfm_weight_pres <- data_corpus_inaugural %>%\n  corpus_subset(Year > 2000) %>%\n  tokens(remove_punct = TRUE) %>%\n  tokens_remove(stopwords(\"english\")) %>%\n  dfm() %>%\n  dfm_weight(scheme = \"prop\")\n\n# Calculate relative frequency by president\nfreq_weight <- textstat_frequency(dfm_weight_pres, n = 10, \n                                  groups = dfm_weight_pres$President)\n\nggplot(data = freq_weight, aes(x = nrow(freq_weight):1, y = frequency)) +\n  geom_point() +\n  facet_wrap(~ group, scales = \"free\") +\n  coord_flip() +\n  scale_x_continuous(breaks = nrow(freq_weight):1,\n                     labels = freq_weight$feature) +\n  labs(x = NULL, y = \"Relative frequency\")\n\n\n\n# Only select speeches by Obama and Trump\npres_corpus <- corpus_subset(data_corpus_inaugural, \n                             President %in% c(\"Obama\", \"Trump\"))\n\n# Create a dfm grouped by president\npres_dfm <- tokens(pres_corpus, remove_punct = TRUE) %>%\n  tokens_remove(stopwords(\"english\")) %>%\n  tokens_group(groups = President) %>%\n  dfm()\n\n# Calculate keyness and determine Trump as target group\nresult_keyness <- textstat_keyness(pres_dfm, target = \"Trump\")\n\n# Plot estimated word keyness\ntextplot_keyness(result_keyness) \n\n\n\n# Plot without the reference text (in this case Obama)\ntextplot_keyness(result_keyness, show_reference = FALSE)\n\n\n\nlibrary(quanteda.textmodels)\n\n# Irish budget speeches from 2010 (data from quanteda.textmodels)\n# Transform corpus to dfm\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\nie_dfm <- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores\nrefscores <- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n# Predict Wordscores model\nws <- textmodel_wordscores(ie_dfm, y = refscores, smooth = 1)\n\n# Plot estimated word positions (highlight words and print them in red)\ntextplot_scale1d(ws,\n                 highlighted = c(\"minister\", \"have\", \"our\", \"budget\"), \n                 highlighted_color = \"red\")\n\n\n\n# Get predictions\npred <- predict(ws, se.fit = TRUE)\n\n# Plot estimated document positions and group by \"party\" variable\ntextplot_scale1d(pred, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n# Plot estimated document positions using the LBG transformation and group by \"party\" variable\n\npred_lbg <- predict(ws, se.fit = TRUE, rescaling = \"lbg\")\n\ntextplot_scale1d(pred_lbg, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n# Estimate Wordfish model\nlibrary(\"quanteda.textmodels\")\nwf <- textmodel_wordfish(dfm(tokens(data_corpus_irishbudget2010)), dir = c(6, 5))\n\n# Plot estimated word positions\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n# Plot estimated document positions\ntextplot_scale1d(wf, groups = data_corpus_irishbudget2010$party)\n\n\n\n# Transform corpus to dfm\nie_dfm <- dfm(tokens(data_corpus_irishbudget2010))\n\n# Run correspondence analysis on dfm\nca <- textmodel_ca(ie_dfm)\n\n# Plot estimated positions and group by party\ntextplot_scale1d(ca, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\nWhat is wordfish?\na program on R used for extracting political positions from text documents."
  },
  {
    "objectID": "Assignment5.html",
    "href": "Assignment5.html",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "",
    "text": "## Collecting Social Media data: YouTube\n\n# Required Libraries\n# Install if necessary\n#install.packages(\"tuber\")\n#install.packages(\"tidyverse\")\n#install.packages(\"lubridate\")\n#install.packages(\"stringi\")\n#install.packages(\"wordcloud\")\n#install.packages(\"gridExtra\")\n#install.packages(\"httr\")\n#install.packages(\"tm\")\n\n\n\nlibrary(tuber)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(stringi)\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(httr)\nlibrary(tm)\n\nLoading required package: NLP\n\nAttaching package: 'NLP'\n\nThe following object is masked from 'package:httr':\n\n    content\n\nThe following object is masked from 'package:ggplot2':\n\n    annotate\n\n### Step 1: Apply for the Google YouTube API\n\n\n### 1.  Go to Google Cloud Console (https://cloud.google.com/).\n### 2.  Create a new project or select an existing one.\n### 3.  Search for and enable the YouTube Data API v3.\n### 4.  Go to Credentials > Create Credentials > OAuth Client ID.\n### 5.  Set up the OAuth consent screen\n### 6.  Name the App (e.g. YouTube analyzer), enter support email\n### 7.  Application type: web application\n### 8.  Name: Web client 1\n### 9.  Generate Client ID and Client Secret.\n\n\n### Step 2: Authenticate with YouTube API\n\n#### Use your Client ID and Client Secret to authenticate.\n\n# Replace with your actual Client ID and Client Secret\nyt_oauth(\"645739575442-j8oucmudgt0hmlf61pk4j46k0pe6gkhb.apps.googleusercontent.com\", \"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\", token = \"\")\n\n### Important:  when running for first time, you will be prompted to:\n### 1. add the .httr-oauth to .gitignore, select 1 to consent. \n### 2. Then it will open browser to choose your Google account.  \n### 3. When prompted with safety statement (\"Google hasn’t verified this app\"), click advanced and click Go to Appname (unsafe) to verify.  \n### 4. When done, the message will show \"Authentication complete. Please close this page and return to R.\"\n### 5. Return to RStudio.  When seeing:\n###  \n### \"Waiting for authentication in browser...\n### Press Esc/Ctrl + C to abort\n### Authentication complete.\n### \n### It is ready to collect YouTube data\n\n\n### Step 3: Download YouTube Data\n\n#### Here’s an example of collecting data on the “US election 2024.”\n\n#### Search for videos related to \"US election 2024\"\nyt_CNN <- yt_search(term = \"CNN\")\n\n\n#### Display the first few rows\n\nhead(yt_CNN)\n\n     video_id          publishedAt                channelId\n1 YNH2L5uKt8M 2024-11-30T00:14:41Z UCupvZG-5ko_eiXAupbDfxWw\n2 hMKA7MRqDnc 2024-12-04T02:00:24Z UCupvZG-5ko_eiXAupbDfxWw\n3 jF8BGvXJ_Ps 2024-11-29T18:11:55Z UCupvZG-5ko_eiXAupbDfxWw\n4 1b5zKsrUW0M 2024-12-04T03:20:00Z UCupvZG-5ko_eiXAupbDfxWw\n5 ecxdbWtLYNE 2024-12-03T20:30:32Z UCupvZG-5ko_eiXAupbDfxWw\n6 xtZnqz3djBg 2024-12-04T00:59:05Z UCupvZG-5ko_eiXAupbDfxWw\n                                                                                                 title\n1                                                       Trump having dinner with Trudeau at Mar-a-Lago\n2                                       New video of violent police clashes with anti-Putin protestors\n3                                Elon Musk and Trump dance to &#39;Y-M-C-A&#39; at Thanksgiving dinner\n4     Republican strategist makes prediction about whether Trump will ‘cut’ Hegseth ‘free’ as his pick\n5                         Yoon backtracks after plunging South Korea into chaos with martial law order\n6 Protestors call for South Korean President Yoon Suk Yeol to step down after historic political chaos\n                                                                                                                             description\n1     Canadian Prime Minister Justin Trudeau arrived in West Palm Beach, Florida to have dinner with President-elect Donald Trump at ...\n2              CNN's Matthew Chance reports on six days of clashes between anti-Putin protesters and police in Georgia following the ...\n3               Tesla CEO Elon Musk celebrated Thanksgiving with President-elect Donald Trump and his family at Mar-A-Lago. Musk was ...\n4   As one of President-elect Donald Trump's cabinet picks appears to have an uphill battle in a confirmation hearing, Pete Hegseth, ...\n5 CNN's Mike Valerio reports from Seoul after South Korean President Yoon Suk Yeol said he will lift his martial law order following ...\n6    South Korean President Yoon Suk Yeol lifted a martial law order, just hours after his decree plunged the country into political ...\n                          thumbnails.default.url thumbnails.default.width\n1 https://i.ytimg.com/vi/YNH2L5uKt8M/default.jpg                      120\n2 https://i.ytimg.com/vi/hMKA7MRqDnc/default.jpg                      120\n3 https://i.ytimg.com/vi/jF8BGvXJ_Ps/default.jpg                      120\n4 https://i.ytimg.com/vi/1b5zKsrUW0M/default.jpg                      120\n5 https://i.ytimg.com/vi/ecxdbWtLYNE/default.jpg                      120\n6 https://i.ytimg.com/vi/xtZnqz3djBg/default.jpg                      120\n  thumbnails.default.height                            thumbnails.medium.url\n1                        90 https://i.ytimg.com/vi/YNH2L5uKt8M/mqdefault.jpg\n2                        90 https://i.ytimg.com/vi/hMKA7MRqDnc/mqdefault.jpg\n3                        90 https://i.ytimg.com/vi/jF8BGvXJ_Ps/mqdefault.jpg\n4                        90 https://i.ytimg.com/vi/1b5zKsrUW0M/mqdefault.jpg\n5                        90 https://i.ytimg.com/vi/ecxdbWtLYNE/mqdefault.jpg\n6                        90 https://i.ytimg.com/vi/xtZnqz3djBg/mqdefault.jpg\n  thumbnails.medium.width thumbnails.medium.height\n1                     320                      180\n2                     320                      180\n3                     320                      180\n4                     320                      180\n5                     320                      180\n6                     320                      180\n                               thumbnails.high.url thumbnails.high.width\n1 https://i.ytimg.com/vi/YNH2L5uKt8M/hqdefault.jpg                   480\n2 https://i.ytimg.com/vi/hMKA7MRqDnc/hqdefault.jpg                   480\n3 https://i.ytimg.com/vi/jF8BGvXJ_Ps/hqdefault.jpg                   480\n4 https://i.ytimg.com/vi/1b5zKsrUW0M/hqdefault.jpg                   480\n5 https://i.ytimg.com/vi/ecxdbWtLYNE/hqdefault.jpg                   480\n6 https://i.ytimg.com/vi/xtZnqz3djBg/hqdefault.jpg                   480\n  thumbnails.high.height channelTitle liveBroadcastContent          publishTime\n1                    360          CNN                 none 2024-11-30T00:14:41Z\n2                    360          CNN                 none 2024-12-04T02:00:24Z\n3                    360          CNN                 none 2024-11-29T18:11:55Z\n4                    360          CNN                 none 2024-12-04T03:20:00Z\n5                    360          CNN                 none 2024-12-03T20:30:32Z\n6                    360          CNN                 none 2024-12-04T00:59:05Z\n\n### Step 4: Basic Analytics on YouTube Data\n\n#### Most Frequent Words in Video Titles\n\n# Extract titles and clean up\n\n# Extract titles and clean up\ntitles <- yt_CNN$title\ntitles_clean <- tolower(titles) %>%\n  stri_replace_all_regex(\"[[:punct:]]\", \"\") %>%\n  str_split(\" \") %>%\n  unlist()\n\n# Create a word frequency table\nword_freq <- table(titles_clean)\nword_freq_df <- as.data.frame(word_freq, stringsAsFactors = FALSE)\ncolnames(word_freq_df) <- c(\"word\", \"freq\")\n\n# Filter common words (stop words) and plot a word cloud\nword_freq_df <- word_freq_df %>% filter(!word %in% tm::stopwords(\"en\"))\nset.seed(123)\nwordcloud(words = word_freq_df$word, freq = word_freq_df$freq, max.words = 50)\n\n\n\n### 4.2. Plot Video Publish Dates\n\n# Format publish dates and aggregate data\n\n\nyt_sm <- yt_CNN %>%\n  mutate(publish_date = as.Date(publishedAt)) %>%\n  count(publish_date)\n\n# Plot the frequency of videos published over time\nggplot(yt_sm, aes(x = publish_date, y = n)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Videos Published Over Time\", x = \"Date\", y = \"Number of Videos\") +  \n  theme_bw()\n\n\n\n###  4.3. Top Channels by Video Count\n\n\n# Summarize by channel\ntop_channels <- yt_CNN %>%\n  count(channelTitle, sort = TRUE) %>%\n  top_n(10)\n\nSelecting by n\n\n# Plot top channels\nggplot(top_channels, aes(x = reorder(channelTitle, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"red\") +\n  coord_flip() +\n  labs(title = \"Top Channels on 'CNN'\", x = \"Channel\", y = \"Number of Videos\")\n\n\n\n\n\nThe word “cnn” and “trump” comment stands out more because of the freqeuncy\nThe CNN channel stats shows us the top channels with CNN having the most number of videos\nCan you use quanteda to analyze the text data from Youtube comments?\nYes, you can use quanteda to analyze text data. It will show you the results in a bar graph"
  },
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "",
    "text": "The survey has different options in which the user can interact with which include a list view, dropdown view, or select box view."
  },
  {
    "objectID": "Assignment1.html#what-is-the-questionnaire-composed-of",
    "href": "Assignment1.html#what-is-the-questionnaire-composed-of",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "What is the questionnaire composed of?",
    "text": "What is the questionnaire composed of?\nThroughout the survey there are numerous multiple choice questions, text entry, and matrix table."
  },
  {
    "objectID": "Assignment1.html#how-are-the-questions-ordered",
    "href": "Assignment1.html#how-are-the-questions-ordered",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "How are the questions ordered?",
    "text": "How are the questions ordered?\nThe questions are ordered numerically."
  },
  {
    "objectID": "Assignment4.html",
    "href": "Assignment4.html",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "",
    "text": "## Workshop: Scraping webpages with R rvest package\n# Prerequisites: Chrome browser, Selector Gadget\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\"))\n\ninstall.packages(\"tidyverse\")\n\n\nThe downloaded binary packages are in\n    /var/folders/s4/3ms31zss3gg_5z9j8pfqmxl00000gn/T//Rtmp4onKVT/downloaded_packages\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\ninstall.packages(\"rvest\")\n\n\nThe downloaded binary packages are in\n    /var/folders/s4/3ms31zss3gg_5z9j8pfqmxl00000gn/T//Rtmp4onKVT/downloaded_packages\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nurl <- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n#Reading the HTML code from the Wiki website\nclass(url)\n\n[1] \"character\"\n\nwikiforreserve <- read_html(url)\nclass(wikiforreserve)\n\n[1] \"xml_document\" \"xml_node\"    \n\n## Get the XPath data using Inspect element feature in Safari, Chrome or Firefox\n## At Inspect tab, look for <table class=....> tag. Leave the table close\n## Right click the table and Copy --> XPath, paste at html_nodes(xpath =)\n\nforeignreserve <- wikiforreserve %>%\n  html_nodes(xpath='//*[@id=\"mw-content-text\"]/div[1]/table[1]') %>%\n  html_table()\nclass(foreignreserve) # Why the first column is not scrapped?\n\n[1] \"list\"\n\nfores = foreignreserve[[1]][,c(1, 2,3,4,5,6,7) ] # [[ ]] returns a single element directly, without retaining the list structure.\n\n\n# \nnames(fores) <- c(\"Country\", \"Forexreswithgold\", \"Date1\", \"Change1\",\"Forexreswithoutgold\", \"Date2\",\"Change2\", \"Sources\")\n\nWarning: The `value` argument of `names<-()` must have the same length as `x` as of\ntibble 3.0.0.\n\ncolnames(fores)\n\n[1] \"Country\"             \"Forexreswithgold\"    \"Date1\"              \n[4] \"Change1\"             \"Forexreswithoutgold\" \"Date2\"              \n[7] \"Change2\"            \n\nhead(fores$Country, n=10)\n\n [1] \"Country(as recognized by the U.N.)\" \"China\"                             \n [3] \"Japan\"                              \"Switzerland\"                       \n [5] \"India\"                              \"Russia\"                            \n [7] \"Taiwan\"                             \"Saudi Arabia\"                      \n [9] \"Hong Kong\"                          \"South Korea\"                       \n\nclass(fores$Date1)\n\n[1] \"character\"\n\n# Sources column useful?\n\n## Clean up variables\n## What type is Date?\n\n# Convert Date1 variable\nfores$Date1 = as.Date(fores$Date1, format = \"%d %b %Y\")\nclass(fores$Date1)\n\n[1] \"Date\"\n\nwrite.csv(fores, \"fores.csv\", row.names = FALSE) # use fwrite?\n\n\n## Chizoma's modified Code Script for Scraping Wikipedia Tables (Foreign Exchange Specific)\n\ninstall.packages(\"tidyverse\")\n\n\nThe downloaded binary packages are in\n    /var/folders/s4/3ms31zss3gg_5z9j8pfqmxl00000gn/T//Rtmp4onKVT/downloaded_packages\n\nlibrary(tidyverse)\ninstall.packages(\"rvest\")\n\n\nThe downloaded binary packages are in\n    /var/folders/s4/3ms31zss3gg_5z9j8pfqmxl00000gn/T//Rtmp4onKVT/downloaded_packages\n\nlibrary(rvest)\n\n# Hardcode URL for foreign exchange reserves page\nurl <- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n\n# Read the webpage\nwikidata <- read_html(url)\n\n# Extract all tables\ntables <- wikidata %>%\n  html_nodes(\"table\") %>%\n  html_table(fill = TRUE)\n\n# Display the number of tables\ncat(\"Number of tables found:\", length(tables), \"\\n\")\n\nNumber of tables found: 3 \n\n# Preview tables\nfor (i in seq_along(tables)) {\n  cat(\"\\nTable\", i, \":\\n\")\n  print(head(tables[[i]], 3)) # Print first 3 rows as a preview\n}\n\n\nTable 1 :\n# A tibble: 3 × 10\n  `Country(as recognized by the U.N.)` `U.N. Geoscheme` `U.N. Geoscheme`\n  <chr>                                <chr>            <chr>           \n1 Country(as recognized by the U.N.)   Continent        Sub-region      \n2 China                                Asia             East Asia       \n3 Japan                                Asia             East Asia       \n# ℹ 7 more variables: `Forex reserves including gold` <chr>,\n#   `Forex reserves including gold` <chr>,\n#   `Forex reserves including gold` <chr>,\n#   `Forex reserves excluding gold` <chr>,\n#   `Forex reserves excluding gold` <chr>,\n#   `Forex reserves excluding gold` <chr>, Ref. <chr>\n\nTable 2 :\n# A tibble: 3 × 14\n  ``                      `` ``    Currency composition…¹ Currency composition…²\n  <chr>                <int> <chr> <chr>                  <chr>                 \n1 \"\"                      NA \"\"    USD                    EUR                   \n2 \"@supports(writing-…  2019 \"Q1\"  6,727.09               2,208.79              \n3 \"@supports(writing-…  2019 \"Q2\"  6,752.28               2,264.88              \n# ℹ abbreviated names:\n#   ¹​`Currency composition of foreign exchange reserves (COFER) (billion U.S$.)[212]`,\n#   ²​`Currency composition of foreign exchange reserves (COFER) (billion U.S$.)[212]`\n# ℹ 9 more variables:\n#   `Currency composition of foreign exchange reserves (COFER) (billion U.S$.)[212]` <chr>,\n#   `Currency composition of foreign exchange reserves (COFER) (billion U.S$.)[212]` <chr>,\n#   `Currency composition of foreign exchange reserves (COFER) (billion U.S$.)[212]` <chr>, …\n\nTable 3 :\n# A tibble: 3 × 2\n  .mw-parser-output .navbar{display:inline;font-size:88…¹ .mw-parser-output .n…²\n  <chr>                                                   <chr>                 \n1 Trade                                                   \"Account balance\\n% o…\n2 Investment                                              \"FDI received\\npast\\n…\n3 Funds                                                   \"Forex reserves\\nGold…\n# ℹ abbreviated names:\n#   ¹​`.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}vteLists of countries by financial rankings`,\n#   ²​`.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}vteLists of countries by financial rankings`\n\n# Select the first table (foreign exchange reserves data)\ntable_index <- 1\nselected_table <- tables[[table_index]]\n\n# Clean column names\nnames(selected_table) <- c(\"Country\", \"ForexResWithGold\", \"Date1\", \"Change1\",\n                           \"ForexResWithoutGold\", \"Date2\", \"Change2\", \"Sources\")\n\nWarning: The `value` argument of `names<-()` can't be empty as of tibble 3.0.0.\n\n# Optional: Additional cleanup for the \"foreign exchange reserves\" table\nselected_table <- selected_table[, c(1, 2, 3, 4, 5, 6, 7)]  # Drop \"Sources\" column\n\n# Convert Date1 column to proper date format\nselected_table$Date1 <- as.Date(selected_table$Date1, format = \"%d %b %Y\")\n\n# Display cleaned table\nprint(head(selected_table))\n\n# A tibble: 6 × 7\n  Country  ForexResWithGold Date1  Change1     ForexResWithoutGold Date2 Change2\n  <chr>    <chr>            <date> <chr>       <chr>               <chr> <chr>  \n1 Country… Continent        NA     U.S.$milli… Lastreporteddate    Chan… U.S.mi…\n2 China    Asia             NA     3,571,803   31 Oct 2024         21,9… 3,380,…\n3 Japan    Asia             NA     1,238,950   1 Nov 2024          15,9… 1,164,…\n4 Switzer… Europe           NA     952,687     30 Sep 2024         1,127 864,519\n5 India    Asia             NA     656,580     22 Nov 2024         1,310 588,981\n6 Russia   Europe           NA     620,800     8 Nov 2024          11,9… 428,534\n\n# Save to CSV\nfilename <- \"foreign_exchange_reserves\"\nwrite.csv(selected_table, paste0(filename, \".csv\"), row.names = FALSE)\ncat(\"Table saved as\", paste0(filename, \".csv\"))\n\nTable saved as foreign_exchange_reserves.csv\n\n\n\nWhat type is Date: Date is representing dates without time\nWhy is the first column not scraped correctly? Incorrect merged cells\n\n\n\n\n## Scraping Government data\n## Website: GovInfo (https://www.govinfo.gov/app/search/)\n## Prerequisite: Download from website the list of files to be downloaded\n## Designed for background job\n\n# Start with a clean plate and lean loading to save memory\n\ngc(reset=T)\n\n          used (Mb) gc trigger  (Mb) limit (Mb) max used (Mb)\nNcells 1112547 59.5    2224498 118.9         NA  1112547 59.5\nVcells 2039854 15.6    8388608  64.0      16384  2039854 15.6\n\n#install.packages(c(\"purrr\", \"magrittr\")\nlibrary(purrr)\nlibrary(magrittr) # Alternatively, load tidyverse\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n## Set path for reading the listing and home directory\n## For Windows, use \"c:\\\\directory\\\\subdirectory\\\\\"\n## For Mac, \"/Users/YOURNAME/path/\"\n\nsetwd(\"/Users/coparaji/datacollection.github.io\")\nlibrary(rjson)\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\n\nThe following objects are masked from 'package:rjson':\n\n    fromJSON, toJSON\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(readr)\n\n## CSV method\ngovfiles= read.csv(file=\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_10_42.csv\", skip=2)\n\n## JSON method\n### rjson\ngf_list <- rjson::fromJSON(file =\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\")\ngovfile2=dplyr::bind_rows(gf_list$resultSet)\n\n### jsonlite\ngf_list1 = jsonlite::read_json(\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\")\n\n### Extract the list\ngovfiles3 <- gf_list1$resultSet\n\n### One more step\ngovfiles3 <- gf_list1$resultSet |> dplyr::bind_rows()\n\n\n# Preparing for bulk download of government documents\ngovfiles$id = govfiles$packageId\npdf_govfiles_url = govfiles$pdfLink\npdf_govfiles_id <- govfiles$id\n\n# Directory to save the pdf's\nsave_dir <- \"/Users/coparaji/Downloads.pdf\"\n\n# Function to download pdfs\ndownload_govfiles_pdf <- function(url, id) {\n  tryCatch({\n    destfile <- paste0(save_dir, \"govfiles_\", id, \".pdf\")\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Important: random sleep between 1 and 3 seconds to avoid suspicion of \"hacking\" the server\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n# Download files, potentially in parallel for speed\n# Simple timer, can use package like tictoc\n# \n\n## Try downloading one document\nstart.time <- Sys.time()\nmessage(\"Starting downloads\")\n\nStarting downloads\n\nresults <- 1:1 %>% \n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\n\nWarning in download.file(url, destfile = destfile, mode = \"wb\"): downloaded\nlength 30849141 != reported length 424775211\n\n\nWarning in download.file(url, destfile = destfile, mode = \"wb\"): URL\n'https://www.govinfo.gov/content/pkg/CPRT-95JPRT20039OvIII/pdf/CPRT-95JPRT20039OvIII.pdf':\nTimeout of 60 seconds was reached\n\nmessage(\"Finished downloads\")\n\nFinished downloads\n\nend.time <- Sys.time()\ntime.taken <- end.time - start.time\ntime.taken\n\nTime difference of 1.002118 mins\n\n## Try all five\nstart.time <- Sys.time()\nmessage(\"Starting downloads\")\n\nStarting downloads\n\nresults <- 1:length(pdf_govfiles_url) %>% \n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\n\nWarning in download.file(url, destfile = destfile, mode = \"wb\"): downloaded\nlength 105421382 != reported length 424775211\nWarning in download.file(url, destfile = destfile, mode = \"wb\"): URL\n'https://www.govinfo.gov/content/pkg/CPRT-95JPRT20039OvIII/pdf/CPRT-95JPRT20039OvIII.pdf':\nTimeout of 60 seconds was reached\n\n\nWarning in download.file(url, destfile = destfile, mode = \"wb\"): downloaded\nlength 36660636 != reported length 413579288\n\n\nWarning in download.file(url, destfile = destfile, mode = \"wb\"): URL\n'https://www.govinfo.gov/content/pkg/CPRT-95JPRT20818OvI/pdf/CPRT-95JPRT20818OvI.pdf':\nTimeout of 60 seconds was reached\n\nmessage(\"Finished downloads\")\n\nFinished downloads\n\nend.time <- Sys.time()\ntime.taken <- end.time - start.time\ntime.taken\n\nTime difference of 2.189016 mins\n\n# Print results\nprint(results)\n\n[1] \"Failed to download: https://www.govinfo.gov/content/pkg/CPRT-95JPRT20039OvIII/pdf/CPRT-95JPRT20039OvIII.pdf\"\n[2] \"Failed to download: https://www.govinfo.gov/content/pkg/CPRT-95JPRT20818OvI/pdf/CPRT-95JPRT20818OvI.pdf\"    \n[3] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/CPRT-109JPRT25514/pdf/CPRT-109JPRT25514.pdf\"   \n[4] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/CPRT-115SPRT28545/pdf/CPRT-115SPRT28545.pdf\"   \n[5] \"Successfully downloaded: https://www.govinfo.gov/content/pkg/CPRT-115SPRT23704/pdf/CPRT-115SPRT23704.pdf\"   \n\n## Exercise: Try downloading 118th Congress Congressional Hearings in Committee on Foreign Affairs?"
  },
  {
    "objectID": "Assignment4.html#download-10-documents",
    "href": "Assignment4.html#download-10-documents",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "Download 10 documents",
    "text": "Download 10 documents\n\nhttps://www.govinfo.gov/content/pkg/CPRT-115SPRT23704/pdf/CPRT-115SPRT23704.pdf\nhttps://www.govinfo.gov/content/pkg/CPRT-115SPRT28545/pdf/CPRT-115SPRT28545.pdf\nhttps://www.govinfo.gov/content/pkg/CPRT-109JPRT25514/pdf/CPRT-109JPRT25514.pdf\nhttps://www.govinfo.gov/content/pkg/CPRT-95JPRT20039OvIII/pdf/CPRT-95JPRT20039OvIII.pdf\nhttps://www.govinfo.gov/content/pkg/CPRT-95JPRT20818OvI/pdf/CPRT-95JPRT20818OvI.pdf\nhttps://www.govinfo.gov/content/pkg/CPRT-109JPRT25514/pdf/CPRT-109JPRT25514.pdf\nhttps://www.govinfo.gov/content/pkg/CPRT-115SPRT28545/pdf/CPRT-115SPRT28545.pdf\nhttps://www.govinfo.gov/content/pkg/CPRT-109JPRT25514/pdf/CPRT-109JPRT25514.pdf\nhttps://www.govinfo.gov/content/pkg/CPRT-115SPRT28545/pdf/CPRT-115SPRT28545.pdf\nhttps://www.govinfo.gov/content/pkg/CPRT-115SPRT23704/pdf/CPRT-115SPRT23704.pdf\nhttps://www.govinfo.gov/content/pkg/CPRT-109JPRT25514/pdf/CPRT-109JPRT25514.pdf\nhttps://www.govinfo.gov/content/pkg/CPRT-115SPRT28545/pdf/CPRT-115SPRT28545.pdf\nhttps://www.govinfo.gov/content/pkg/CPRT-115SPRT23704/pdf/CPRT-115SPRT23704.pdf"
  },
  {
    "objectID": "Assignment4.html#web-scrapping-report",
    "href": "Assignment4.html#web-scrapping-report",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "Web scrapping Report",
    "text": "Web scrapping Report\nHaving different formats, for example in csv or json increases the complexity, especially when it comes to having different libraries with compatibility. In the Json structure there is additional parsing steps to convert into tabular formats. Any minor changes in the URL or data structure broke the web scrapping process, especially when trying to update. Lastly, the disk storage. Government data sets are saved in PDFs which can be large, you need adequate disk space prior. To help avoid challanges you can use APIs which is more reliable and stable than scrapping. For URL challenges one can look into automating updates for changing URLs"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "",
    "text": "Hi There!\nI’m Chizoma Oparaji, experienced Data Analyst and UX Researcher with a strong foundation in turning complex data into actionable insights and designing user-centric digital experiences. Combining expertise in data analysis and user research, I leverage both quantitative and qualitative methodologies to optimize product design and improve user engagement. Passionate about blending analytical skills with a deep understanding of user behavior to create intuitive, data-driven solutions that meet both business goals and user needs."
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "Skills",
    "text": "Skills\n\nSurveys\nContextual Interviews\nHeuristic Evaluation\nPersonas\nStoryboarding\nWireframing\nData Visualization\nRapid Prototyping\nDesign Thinking\nInformation Architecture\nWeb Design\nResponsive Design\nCompetitive Analysis\nAffinity Diagramming\nCustomer Journey Mapping\nUsability Testing\nInteraction Design\nUI Design"
  },
  {
    "objectID": "about.html#tools",
    "href": "about.html#tools",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "Tools",
    "text": "Tools\n\nFigma Sketch Invision Miro\nMural Python Javascript\nMarvel Microsoft Suite\nPhotoshop Webflow Illustrator PROGRAMMING\nHTML CSS CERTIFICATIONS\nDscount Tableau Power BI InDesign"
  },
  {
    "objectID": "about.html#contact-me",
    "href": "about.html#contact-me",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "Contact Me",
    "text": "Contact Me\nhttps://chizo14.github.io/datacollection.github.io"
  },
  {
    "objectID": "Coursework.html",
    "href": "Coursework.html",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "",
    "text": "This course introduces the database systems including the concepts, design, programming, and management of database systems. It provides training in DBMS and introduces new technologies such as NoSQL and various types of modern database systems.process."
  },
  {
    "objectID": "Coursework.html#data-visualization",
    "href": "Coursework.html#data-visualization",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "Data Visualization",
    "text": "Data Visualization\nThis course builds data literacy through communicating data patterns, findings and insights via visual representation of data. This course is designed to equip data scientists with data theory, principles and concepts of visualizing data and best practices in visual data analytics."
  },
  {
    "objectID": "Coursework.html#methods-of-data-collection-and-production",
    "href": "Coursework.html#methods-of-data-collection-and-production",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "Methods of Data Collection and Production",
    "text": "Methods of Data Collection and Production\nThis course introduces data collection and production methods in the big data age. It aims at providing a comprehensive framework in understanding data, and how social scientists conduct research starting from the data generation process."
  },
  {
    "objectID": "Coursework.html#research-in-neuroscience",
    "href": "Coursework.html#research-in-neuroscience",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "Research in Neuroscience",
    "text": "Research in Neuroscience\nThis is an internship where I conducted reserach in the field of Cognitve Science and Neuroscience"
  },
  {
    "objectID": "Coursework.html#special-topics-in-cognitive-science-and-neuroscience",
    "href": "Coursework.html#special-topics-in-cognitive-science-and-neuroscience",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "Special Topics in Cognitive Science and Neuroscience",
    "text": "Special Topics in Cognitive Science and Neuroscience\nThis is a special topics class where I got the opportunity to build a Virtual Reality driving simulation model using Python, SQL, and various research methods."
  },
  {
    "objectID": "Coursework.html#user-experience-design",
    "href": "Coursework.html#user-experience-design",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "User Experience Design",
    "text": "User Experience Design\nUnderstanding how to design effective user experiences is essential for the success of a software system. This class covers topics associated with the design and analysis of user interfaces for software systems and explores human-computer interaction."
  },
  {
    "objectID": "Final Paper.html",
    "href": "Final Paper.html",
    "title": "Chizoma Oparaji's Portfolio",
    "section": "",
    "text": "Assessing the Role of Technological Advancement in Shaping Birth Rate Trends in the United States\nSlides"
  }
]